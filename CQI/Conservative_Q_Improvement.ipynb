{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conservative Q-Improvement.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMFCPk1qcUkHYPVzlX1aQBp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoKoum/Inherently-Interpretable-Q-Learning/blob/main/CQI/Conservative_Q_Improvement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/JoKoum/Inherently-Interpretable-Q-Learning.git"
      ],
      "metadata": {
        "id": "cqhke1jL-8iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray\n",
        "!pip install cqi-rl"
      ],
      "metadata": {
        "id": "Za9GWHfil5Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(r'./Inherently-Interpretable-Q-Learning')"
      ],
      "metadata": {
        "id": "V00EdJHm_JWE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KhwaLUDxltmd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import deque, defaultdict\n",
        "from statistics import mean\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from cqi_cpp.src.wrapper.qtree_wrapper import PyAction as Action\n",
        "from cqi_cpp.src.wrapper.qtree_wrapper import PyDiscrete as Discrete\n",
        "from cqi_cpp.src.wrapper.qtree_wrapper import PyQTree as QTree\n",
        "from cqi_cpp.src.wrapper.qtree_wrapper import PyState as State\n",
        "from cqi_cpp.src.wrapper.qtree_wrapper import PyBox as Box\n",
        "from cqi_cpp.src.wrapper.qtree_wrapper import PyVector as Vector\n",
        "import gym\n",
        "import pickle\n",
        "from RobotNavigation.RobotNavigation import RobotNavEnv\n",
        "from utils.scores import ScoreLogger"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_pybox(b):\n",
        "    low = Vector()\n",
        "    high = Vector()\n",
        "\n",
        "    for i in b.low: low.add(i)\n",
        "\n",
        "    for i in b.high: high.add(i)\n",
        "\n",
        "    return Box(low, high)\n",
        "\n",
        "def convert_to_pystate(s):\n",
        "    if type(s) is State: return s\n",
        "\n",
        "    v = Vector()\n",
        "\n",
        "    for i in s: v.add(i)\n",
        "\n",
        "    return State(v)\n",
        "\n",
        "class StateChangeTracker(object):\n",
        "    def __init__(self, init_states):\n",
        "        self.init_states = init_states\n",
        "        self.final_states = init_states\n",
        "        self.single_change = [False] * len(init_states)\n",
        "\n",
        "    def new_states(self, s):\n",
        "        # Capture First Change True, Second+ Change False, No change - keep same\n",
        "        self.single_change = [((i[0] != i[2] and i[1] == i[2]) or (i[0] == i[1] and i[3]))\n",
        "                              for i in zip(s, self.final_states, self.init_states, self.single_change)]\n",
        "        self.final_states = s\n",
        "\n",
        "\n",
        "class Train(object):\n",
        "    def __init__(self, qfunc, gym_env, expl_data_filename=\"explain_data.csv\"):\n",
        "        self.qfunc = qfunc\n",
        "        self.env = gym_env\n",
        "        self.expl_data_filename = expl_data_filename\n",
        "        self._self_tree_ct = 0\n",
        "        self._pickle_filename = \"qfunc_copy_%d.pkl\"\n",
        "\n",
        "    def note_expla_data(self, tag, nodes, reward):\n",
        "        with open(self.expl_data_filename, \"a\") as myfile:\n",
        "            myfile.write(f\"\\n{tag},{nodes},{reward}\")\n",
        "\n",
        "    def train(self, num_steps, eps_func, score_logger, verbose=False, eval_only=False, penalty_check=lambda s, r: 0,\n",
        "              track_data_per=0, run_tag=\"?\", qfunc_hist=None, qfunc_hist_directory=None,\n",
        "              qfunc_hist_per_every_nn=1,\n",
        "              while_watch=False):\n",
        "        if eval_only:\n",
        "            print(\"DEbUG DEbuG EVAL ONLY\")\n",
        "            self.qfunc.print_structure()\n",
        "        hist = defaultdict(list)   # number of nodes, reward per ep\n",
        "        ep_r = 0\n",
        "        done = True\n",
        "        sct = None\n",
        "        r_per_ep = []\n",
        "        pen_per_ep = []\n",
        "        ts_per_ep = []\n",
        "        num_eps = 0\n",
        "        last_step_ep = -1\n",
        "        single_change_features = None\n",
        "        istates, fstates = None, None\n",
        "        for step in range(num_steps):\n",
        "            if done:\n",
        "                if verbose:\n",
        "                    print(f\"Episode reward: {ep_r}; Elapsed steps: {step}\")\n",
        "                    if while_watch and step > 0:\n",
        "                        print(\"Any single state changes?\")\n",
        "                        print(sct.single_change)\n",
        "                        print(sct.init_states)\n",
        "                        print(sct.final_states)\n",
        "                if while_watch and step > 0:\n",
        "                    single_change_features = (single_change_features and\n",
        "                                              sct.single_change) if single_change_features else sct.single_change\n",
        "                    istates = sct.init_states\n",
        "                    fstates = sct.final_states\n",
        "                if eval_only and step > 0:\n",
        "                    r_per_ep.append(ep_r)\n",
        "                    pen_per_ep.append(penalty_check(s, ep_r))\n",
        "                    ts = step - last_step_ep\n",
        "                    ts_per_ep.append(ts)\n",
        "                    last_step_ep = step\n",
        "                if track_data_per > 0 and num_eps % track_data_per == 0:\n",
        "                    hist[self.qfunc.num_nodes()].append(ep_r)\n",
        "                    num_nodes = self.qfunc.num_nodes()\n",
        "                    print(f\"At episode {num_eps}, hist is {num_nodes}, reward is {ep_r}\")\n",
        "                    self.note_expla_data(run_tag, num_nodes, ep_r)\n",
        "                s = self.env.reset()\n",
        "                score_logger.add_score(int(ep_r), num_eps)\n",
        "                ep_r = 0\n",
        "                num_eps = num_eps + 1\n",
        "                done = False\n",
        "                if while_watch:\n",
        "                    sct = StateChangeTracker(s)\n",
        "                    print(\"--------------------A NEW EP BEGINS------------------------------\")\n",
        "            # ∆ê-greedy action selection \n",
        "            if np.random.random() < eps_func(step):\n",
        "                a = self.env.action_space.sample()\n",
        "            else:\n",
        "                s = convert_to_pystate(s)\n",
        "                a = self.qfunc.select_a(s)\n",
        "            s2, r, done, _ = self.env.step(a)\n",
        "            if while_watch:\n",
        "                sct.new_states(s2)\n",
        "            if not eval_only:\n",
        "                s, s2 = convert_to_pystate(s), convert_to_pystate(s2)\n",
        "                a = Action(a)\n",
        "\n",
        "                self.qfunc.take_tuple(s, a, r, s2, done)\n",
        "                if qfunc_hist is not None and self.qfunc.just_split():\n",
        "                    qfunc_hist.append(self.qfunc.get_pre_split())\n",
        "                if qfunc_hist_directory is not None and self.qfunc.just_split():\n",
        "                    tree_pre_split = self.qfunc.get_pre_split()\n",
        "                    nn = tree_pre_split.num_nodes()\n",
        "                    if nn < 10 or nn % qfunc_hist_per_every_nn == 0:\n",
        "                        self._self_tree_ct = self._self_tree_ct + 1\n",
        "                        with open(os.path.join(qfunc_hist_directory, (self._pickle_filename % self._self_tree_ct)), 'wb') as pfile:\n",
        "                            sf = tree_pre_split.sfuncs\n",
        "                            tree_pre_split.sfuncs = None\n",
        "                            pickle.dump(tree_pre_split, pfile)\n",
        "                            tree_pre_split.sfuncs = sf\n",
        "            s = s2\n",
        "            ep_r += r\n",
        "        if eval_only:\n",
        "            avg_r_per_ep = np.mean(r_per_ep)\n",
        "            results = f\"Num_eps: {num_eps}\\nReward per ep: {avg_r_per_ep}\\nTimesteps per ep: {np.mean(ts_per_ep)}\\nPenalties per ep: {np.mean(pen_per_ep)}\"\n",
        "            print(results)\n",
        "            return results, hist, avg_r_per_ep, (single_change_features, istates, fstates)\n",
        "        else:\n",
        "            return hist\n"
      ],
      "metadata": {
        "id": "9kI6WpGUl9vl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "discrete = Discrete(env.action_space.n)\n",
        "\n",
        "box = convert_to_pybox(env.observation_space)\n",
        "\n",
        "qfunc = QTree(box, discrete, None, \n",
        "    gamma=0.99, \n",
        "    alpha=0.1, \n",
        "    visit_decay=0.999, \n",
        "    split_thresh_max=10, \n",
        "    split_thresh_decay=0.99, \n",
        "    num_splits=3)\n",
        "\n",
        "t = Train(qfunc, env)\n",
        "\n",
        "eps_func = (lambda step: max(0.01, 1 - step/1e5))\n",
        "\n",
        "params = {\n",
        "    'method':'Conservative Q-improvement',\n",
        "    'env_name':'CartPole-v1',\n",
        "    'gamma': 0.99,\n",
        "    'learning_rate':0.01,\n",
        "    'eps': '1',\n",
        "    'eps_decay': ' step/1e5',\n",
        "    'eps_min': 0.01,\n",
        "    'batch_size': 'N/A',\n",
        "    'bins': 'N/A',\n",
        "    'epochs': 'N/A',\n",
        "    'category': 'ConservativeQ-Improvement',\n",
        "    'prioritized_experience_replay': False,\n",
        "    'target_model_updates': 0\n",
        "    }\n",
        "\n",
        "score_logger = ScoreLogger(params)\n",
        "\n",
        "# Training\n",
        "t.train(15_000, eps_func, score_logger=score_logger, while_watch=True)\n",
        "\n",
        "# Evaluation:\n",
        "t.train(1000, lambda step: 0.05, score_logger=score_logger, eval_only=True, track_data_per=1)"
      ],
      "metadata": {
        "id": "e9555o4ymzNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "env = RobotNavEnv()\n",
        "\n",
        "discrete = Discrete(env.action_space.n)\n",
        "\n",
        "box = convert_to_pybox(env.observation_space)\n",
        "\n",
        "qfunc = QTree(box, discrete, None, \n",
        "    gamma=0.99, \n",
        "    alpha=0.01, \n",
        "    visit_decay=0.999, \n",
        "    split_thresh_max=10, \n",
        "    split_thresh_decay=0.99, \n",
        "    num_splits=3)\n",
        "\n",
        "t = Train(qfunc, env)\n",
        "\n",
        "eps_func = (lambda step: max(0.01, 1 - step/1e5))\n",
        "\n",
        "params = {\n",
        "    'method':'Conservative Q-improvement',\n",
        "    'env_name':'RobotNavigation',\n",
        "    'gamma': 0.99,\n",
        "    'learning_rate':0.01,\n",
        "    'eps': '1',\n",
        "    'eps_decay': ' step/1e5',\n",
        "    'eps_min': 0.01,\n",
        "    'batch_size': 'N/A',\n",
        "    'bins': 'N/A',\n",
        "    'epochs': 'N/A',\n",
        "    'category': 'ConservativeQ-Improvement',\n",
        "    'prioritized_experience_replay': False,\n",
        "    'target_model_updates': 0\n",
        "    }\n",
        "\n",
        "score_logger = ScoreLogger(params)\n",
        "\n",
        "# Training\n",
        "t.train(10_000, eps_func, score_logger=score_logger, while_watch=True)\n",
        "\n",
        "# Evaluation:\n",
        "t.train(1000, lambda step: 0.05, score_logger=score_logger, eval_only=True, track_data_per=1)"
      ],
      "metadata": {
        "id": "pjrKH5zTm2Nv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}